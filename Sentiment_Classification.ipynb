{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Sentiment_Classification.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRjPv-JffdoW"
      },
      "source": [
        "**This tutorial is based on/ copied from http://peterbloem.nl/blog/transformers.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSW89sPwgOoD"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8joHomDfdoR"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "!pip install pytorch_lightning\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI8VK1S6fdod"
      },
      "source": [
        "# Transformer Implementation\n",
        "From previous exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFpfKjU4fdoi"
      },
      "source": [
        "The transformer architecture consists of multiple transformer blocks that typically look like this: \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/leox1v/dl20/b3d5b5556d1b2bd360a4abeef4fd82f056ab0301/imgs/transformer-block.svg?token=AD5WN2SZYWM6XGH5SXMZM7S7VU3H4\" alt=\"drawing\" width=\"500\"/>\n",
        "\n",
        "\n",
        "It combines a self attention layer, [layer normalization](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html), a feed forward layer and another layer normalization. Additionally, it uses residual connections around the self attention and feed forward layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KH2A6lTfdoe"
      },
      "source": [
        "# Let's implement a SelfAttention torch module.\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A SelfAttention model.\n",
        "    \n",
        "    Args:\n",
        "        d: The embedding dimension.\n",
        "        heads: The number of attention heads.\n",
        "    \"\"\"\n",
        "    def __init__(self, d: int, heads: int=8):\n",
        "        super().__init__()\n",
        "        self.k, self.h = d, heads\n",
        "        \n",
        "        self.Wq = nn.Linear(d, d * heads, bias=False)\n",
        "        self.Wk = nn.Linear(d, d * heads, bias=False)\n",
        "        self.Wv = nn.Linear(d, d * heads, bias=False)\n",
        "        \n",
        "        # This unifies the outputs of the different heads into \n",
        "        # a single k-dimensional vector.\n",
        "        self.unifyheads = nn.Linear(heads * d, d)\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: The input embedding of shape [b, l, d].\n",
        "            \n",
        "        Returns:\n",
        "            Self attention tensor of shape [b, l, d].\n",
        "        \"\"\"\n",
        "        b, l, d = x.size()\n",
        "        h = self.h\n",
        "        \n",
        "        # Transform the input embeddings x of shape [b, l, d] to queries, keys, values.\n",
        "        # The output shape is [b, l, d, d*h] which we transform into [b, l, h, d]. Then,\n",
        "        # we fold the heads into the batch dimenstion to arrive at [b*h, l, d]\n",
        "        queries = self.Wq(x).view(b, l, h, d).transpose(1, 2).contiguous().view(b * h, l, d)\n",
        "        keys = self.Wk(x).view(b, l, h, d).transpose(1, 2).contiguous().view(b * h, l, d)\n",
        "        values = self.Wv(x).view(b, l, h, d).transpose(1, 2).contiguous().view(b * h, l, d)\n",
        "        \n",
        "        # Compute the product of queries and keys and scale with sqrt(d).\n",
        "        # The tensor w' has shape (b*h, l, l) containing raw weights.\n",
        "        w_prime = torch.bmm(queries, keys.transpose(1, 2)) / np.sqrt(d)\n",
        "\n",
        "        # Compute w by normalizing w' over the last dimension.\n",
        "        w = F.softmax(w_prime, dim=-1) \n",
        "        \n",
        "        # Apply the self attention to the values.\n",
        "        out = torch.bmm(w, values).view(b, h, l, d)\n",
        "        \n",
        "        # Swap h, l back.\n",
        "        out = out.transpose(1, 2).contiguous().view(b, l, h * d)\n",
        "        \n",
        "        # Unify heads to arrive at shape [b, l, d].\n",
        "        return self.unifyheads(out)\n",
        "  \n",
        "  \n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A Transformer block consisting of self attention and ff-layer.\n",
        "    \n",
        "    Args:\n",
        "        d (int): The embedding dimension.\n",
        "        heads (int): The number of attention heads.\n",
        "        n_mlp (int): The number of mlp 'blocks'.\n",
        "    \"\"\"\n",
        "    def __init__(self, d: int, heads: int=8, n_mlp: int=4):\n",
        "        super().__init__()\n",
        "        \n",
        "        # The self attention layer.\n",
        "        self.attention = SelfAttention(d, heads=heads)\n",
        "        \n",
        "        # The two layer norms.\n",
        "        self.norm1 = nn.LayerNorm(d)\n",
        "        self.norm2 = nn.LayerNorm(d)\n",
        "        \n",
        "        # The feed-forward layer.\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d, n_mlp*d),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_mlp*d, d)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: The input embedding of shape [b, l, d].\n",
        "            \n",
        "        Returns:\n",
        "            Transformer output tensor of shape [b, l, d].\n",
        "        \"\"\"\n",
        "        x_prime = self.attention(x)\n",
        "        x = self.norm1(x_prime + x)\n",
        "        \n",
        "        x_prime = self.ff(x)\n",
        "        return self.norm2(x_prime + x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqoyEb78gazb"
      },
      "source": [
        "# IMDB Movie Reviews Sentiment Classification\n",
        "We train a Transformer classification model to predict the sentiment (positive, negative) of movie reviews in the [IMDB dataset](https://keras.io/api/datasets/imdb/).\n",
        "\n",
        "We use the [pytorch lightning framework](https://pytorch-lightning.readthedocs.io/en/latest/) to massively reduce the amount of code we need to write."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLcNfrbYfdom"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/leox1v/dl20/b3d5b5556d1b2bd360a4abeef4fd82f056ab0301/imgs/classifier.svg?token=AD5WN2R4NHQQTQFOSQHAVWK7VU3JU\" alt=\"drawing\" width=\"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIB-Q_Tkfdoq"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmYhZoFXfdoq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02907807-1656-4dab-8b71-1af3dc05d259"
      },
      "source": [
        "\n",
        "class IMDBDataModule(pl.LightningDataModule):\n",
        "    \"\"\"\n",
        "    LightningDataModule to load the IMDB movie review sentiment data.\n",
        "    \"\"\" \n",
        "    \n",
        "    def __init__(self, batch_size: int):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "    def setup(self, num_words: int, max_seq_len: int):\n",
        "        \"\"\"\n",
        "        Initial loading of the dataset and transformation.\n",
        "        \n",
        "        Args:\n",
        "            num_words (int): The vocabulary size. The vocabulary is \n",
        "                sorted by frequency of appearance in the dataset.\n",
        "            max_seq_len (int): The maximum number of tokens per\n",
        "                review.\n",
        "        \"\"\"\n",
        "        (self.x_train, self.y_train), (self.x_test, self.y_test) = imdb.load_data(\n",
        "            num_words=num_words, \n",
        "            maxlen=max_seq_len\n",
        "        )\n",
        "        print(f'# Training Examples: {len(self.y_train)}')\n",
        "        print(f'# Test Examples: {len(self.y_test)}')\n",
        "        \n",
        "        self.word2idx = dict(\n",
        "            **{k: v+3 for k, v in imdb.get_word_index().items()},\n",
        "            **{'<PAD>': 0,\n",
        "               '<START>': 1,\n",
        "               '<UNK>': 2,\n",
        "               '<UNUSED>': 3,\n",
        "              },\n",
        "        )\n",
        "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
        "        \n",
        "        # Pad the inputs and convert to torch Tensors.\n",
        "        self.x_train = pad_sequences(self.x_train, maxlen=max_seq_len, value = 0.0)\n",
        "        self.x_test = pad_sequences(self.x_test, maxlen=max_seq_len, value = 0.0)\n",
        "        \n",
        "    \n",
        "    def example(self):\n",
        "        \"\"\"Returns a random training example.\"\"\"        \n",
        "        idx = np.random.randint(0, len(self.x_train))\n",
        "        x, y = self.x_train[idx], self.y_train[idx]\n",
        "        review = ' '.join(self.idx2word[token_id] for token_id in x if token_id > 1)\n",
        "        sentiment = 'POSITIVE' if y else 'NEGATIVE'\n",
        "        return f'{review}\\nSentiment: {sentiment}'\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        dataset = TensorDataset(torch.LongTensor(self.x_train), \n",
        "                                torch.LongTensor(self.y_train))\n",
        "        return DataLoader(dataset, self.batch_size)\n",
        "                                \n",
        "    def test_dataloader(self):\n",
        "        dataset = TensorDataset(torch.LongTensor(self.x_test), \n",
        "                                torch.LongTensor(self.y_test))\n",
        "        return DataLoader(dataset, self.batch_size)\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        dataset = TensorDataset(torch.LongTensor(self.x_test), \n",
        "                                torch.LongTensor(self.y_test))\n",
        "        return DataLoader(dataset, self.batch_size)\n",
        "    \n",
        "imdb_data = IMDBDataModule(128)\n",
        "imdb_data.setup(num_words=30000,\n",
        "                max_seq_len=100)\n",
        "print('\\nExamples:')\n",
        "print('\\n\\n'.join(imdb_data.example() for _ in range(3)))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n",
            "# Training Examples: 2773\n",
            "# Test Examples: 2963\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "\n",
            "Examples:\n",
            "its not braveheart thankfully but it is fine entertainment with engaging characters and good acting all around i enjoyed this film when it was released and upon viewing it again last week find it has held up well over time not a classic film but a very fine and watchable movie to enjoy as great entertainment\n",
            "Sentiment: POSITIVE\n",
            "\n",
            "this is an absolutely incredible film it shows south african racism from the perspective of the victims and provokes a feeling of anti racism in everyone who sees it it is the best historic film i have ever seen\n",
            "Sentiment: POSITIVE\n",
            "\n",
            "this movie really kicked some ass i watched it over and over and it never got boring angelina jolie really kicked some ass in the movie you should see the movie you won't be disappointed and another reason you should see the movie is because the guy from the x files is in it david duchovny\n",
            "Sentiment: POSITIVE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNoR_7ykfdos"
      },
      "source": [
        "class TextClassificationTransformer(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Stacked Transformer blocks for sequence classification.\n",
        "    \n",
        "    Args:\n",
        "        d (int): The embedding dimension.\n",
        "        heads (int): The number of attention heads for each transformer block.\n",
        "        depth (int): The number of transformer blocks.\n",
        "        max_seq_len (int): The maximum number of tokens of each sequence.\n",
        "        num_classes (int): The number of classification classes.\n",
        "    \"\"\"\n",
        "    def __init__(self, d: int=128, heads: int=8, depth: int=6,\n",
        "                max_seq_len: int=512, num_tokens: int=30000, \n",
        "                num_classes: int=2):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        \n",
        "        self.num_tokens = num_tokens\n",
        "        \n",
        "        # Embeddings for tokens and position.\n",
        "        self.token_emb = nn.Embedding(num_tokens, d)\n",
        "        self.pos_emb = nn.Embedding(max_seq_len, d)\n",
        "        \n",
        "        # The stacked transformer blocks.\n",
        "        self.transformer_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(d=d, heads=heads) for _ in range(depth)]\n",
        "        )\n",
        "        \n",
        "        # Mapping of final output sequence to class logits.\n",
        "        self.classification = nn.Linear(d, num_classes)\n",
        "        \n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.accuracy = pl.metrics.Accuracy()\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): A tensor of shape (b, l) of integer values\n",
        "                representing words in some predetermined vocabulary.\n",
        "        \n",
        "        Returns:\n",
        "            A tensor of shape (b, c) of logits over the classes\n",
        "                (c is the number of classes).\n",
        "        \"\"\"\n",
        "        # 1. Generate token embeddings. Shape: [b, l, d].\n",
        "        # 2. Generate position embeddings. Shape: [b, l, d].\n",
        "        # 3. Generate final embedding by taking the sum of the two embeddings.\n",
        "        #----------------\n",
        "        # TODO\n",
        "        #----------------\n",
        "        \n",
        "        # 4. Feed the embedding into the transformer blocks. Shape: [b, l, d].\n",
        "        # 5. Compute the mean latent vector for each sequence.\n",
        "        #    The mean is applied over dim=1 (time). Shape: [b, d].\n",
        "        # 6. Classify. Shape: [b, num_classes].\n",
        "        #----------------\n",
        "        # TODO\n",
        "        #----------------\n",
        "\n",
        "        return x\n",
        "\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        return Adam(self.parameters(), lr=1e-4)\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        \n",
        "        # Forward pass.\n",
        "        logits = self(x)\n",
        "        \n",
        "        # Compute the loss with CrossEntropy.\n",
        "        loss = self.criterion(logits, y)\n",
        "        \n",
        "        # Log the metrics.\n",
        "        self.log('loss', loss, on_epoch=True, prog_bar=True)\n",
        "        self.log('acc', self.accuracy(logits, y), on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        # Lightning automatically disables gradients and puts model in eval mode.\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        \n",
        "        # Log the metrics.\n",
        "        self.log('test_loss', loss, on_epoch=True)\n",
        "        self.log('test_acc', self.accuracy(logits, y), on_epoch=True, prog_bar=True)\n",
        "        \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self.test_step(batch, batch_idx)\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "7s8VE5_zfdou"
      },
      "source": [
        "NUM_WORDS = 10000\n",
        "MAX_SEQ_LEN = 128\n",
        "EMBEDDING_DIM = 128\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "imdb_data = IMDBDataModule(batch_size=BATCH_SIZE)\n",
        "imdb_data.setup(num_words=NUM_WORDS,\n",
        "                max_seq_len=MAX_SEQ_LEN)\n",
        "\n",
        "model = TextClassificationTransformer(d=EMBEDDING_DIM,\n",
        "                                      max_seq_len=MAX_SEQ_LEN,\n",
        "                                      num_tokens=NUM_WORDS)\n",
        "logger = pl.loggers.TensorBoardLogger('tb_logs', name='transformer')\n",
        "trainer = pl.Trainer(max_epochs=5,\n",
        "                     default_root_dir='ckpts',\n",
        "                     gpus=1,\n",
        "                     logger=logger)\n",
        "trainer.fit(model, imdb_data)\n",
        "_ = trainer.test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK7Zqhdl-lR8"
      },
      "source": [
        "## RNN\n",
        "Let's compare our transformer model against an RNN. We want to build a basic RNN architecture as shown in the image below, where we use the output of the final cell to predict the sentiment label. \n",
        "\n",
        "<img src=\"https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment4.png?raw=true\" alt=\"drawing\" width=\"600\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ8Nw_8Q-map"
      },
      "source": [
        "class TextClassificationRNN(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    RNN for sequence classification.\n",
        "    \n",
        "    Args:\n",
        "        d (int): The embedding dimension.\n",
        "        depth (int): The number of RNN blocks.\n",
        "        max_seq_len (int): The maximum number of tokens of each sequence.\n",
        "        num_classes (int): The number of classification classes.\n",
        "    \"\"\"\n",
        "    def __init__(self, d: int=128, depth: int=6,\n",
        "                max_seq_len: int=512, num_tokens: int=30000, \n",
        "                num_classes: int=2):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        \n",
        "        self.num_tokens = num_tokens\n",
        "        \n",
        "        # Embeddings for tokens.\n",
        "        self.token_emb = nn.Embedding(num_tokens, d)\n",
        "        \n",
        "        # The stacked GRU layers.\n",
        "        self.rnn = nn.GRU(input_size=d,\n",
        "                          hidden_size=d,\n",
        "                          num_layers=depth,\n",
        "                          batch_first=True)\n",
        "        \n",
        "        # Mapping of final output sequence to class logits.\n",
        "        self.classification = nn.Linear(d, num_classes)\n",
        "        \n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.accuracy = pl.metrics.Accuracy()\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): A tensor of shape (b, l) of integer values\n",
        "                representing words in some predetermined vocabulary.\n",
        "        \n",
        "        Returns:\n",
        "            A tensor of shape (b, c) of logits over the classes\n",
        "                (c is the number of classes).\n",
        "        \"\"\"\n",
        "        # 1. Generate token embeddings. (No position embedding required for RNNs!) \n",
        "        #     Shape: [b, l, d].\n",
        "        # 2. Feed the embedding into the GRU. Shape: [b, l, d]. Use the output\n",
        "        #     of the last token as the encoding.\n",
        "        # 3. Classify. Shape: [b, num_classes].\n",
        "        #----------------\n",
        "        # TODO\n",
        "        #----------------\n",
        "        return x\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return Adam(self.parameters(), lr=1e-4)\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        \n",
        "        # Forward pass.\n",
        "        logits = self(x)\n",
        "        \n",
        "        # Compute the loss with CrossEntropy.\n",
        "        loss = self.criterion(logits, y)\n",
        "        \n",
        "        # Log the metrics.\n",
        "        self.log('loss', loss, on_epoch=True, prog_bar=True)\n",
        "        self.log('acc', self.accuracy(logits, y), on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        # Lightning automatically disables gradients and puts model in eval mode.\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        \n",
        "        # Log the metrics.\n",
        "        self.log('test_loss', loss, on_epoch=True)\n",
        "        self.log('test_acc', self.accuracy(logits, y), on_epoch=True, prog_bar=True)\n",
        "        \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self.test_step(batch, batch_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrCynVgPCXp-"
      },
      "source": [
        "NUM_WORDS = 10000\n",
        "MAX_SEQ_LEN = 128\n",
        "EMBEDDING_DIM = 512\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "imdb_data = IMDBDataModule(batch_size=BATCH_SIZE)\n",
        "imdb_data.setup(num_words=NUM_WORDS,\n",
        "                max_seq_len=MAX_SEQ_LEN)\n",
        "\n",
        "model = TextClassificationRNN(d=EMBEDDING_DIM,\n",
        "                              max_seq_len=MAX_SEQ_LEN,\n",
        "                              num_tokens=NUM_WORDS)\n",
        "logger = pl.loggers.TensorBoardLogger('tb_logs', name='rnn')\n",
        "trainer = pl.Trainer(max_epochs=5,\n",
        "                     default_root_dir='ckpts',\n",
        "                     gpus=1,\n",
        "                     logger=logger)\n",
        "trainer.fit(model, imdb_data)\n",
        "_ = trainer.test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOPZRkX1IKKn"
      },
      "source": [
        "%tensorboard --logdir tb_logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
