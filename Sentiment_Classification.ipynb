{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Sentiment_Classification.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRjPv-JffdoW"
      },
      "source": [
        "**This tutorial is based on/copied from http://peterbloem.nl/blog/transformers.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSW89sPwgOoD"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8joHomDfdoR"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "!pip install pytorch_lightning\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFpfKjU4fdoi"
      },
      "source": [
        "# Transformer Implementation - Copied from *Transformer* Notebook\n",
        "The transformer architecture consists of multiple transformer blocks that typically look like this: \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/leox1v/dl20/b3d5b5556d1b2bd360a4abeef4fd82f056ab0301/imgs/transformer-block.svg?token=AD5WN2SZYWM6XGH5SXMZM7S7VU3H4\" alt=\"drawing\" width=\"500\"/>\n",
        "\n",
        "\n",
        "It combines a self attention layer, [layer normalization](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html), a feed forward layer and another layer normalization. Additionally, it uses residual connections around the self attention and feed forward layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KH2A6lTfdoe"
      },
      "source": [
        "# Let's implement a SelfAttention torch module.\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A SelfAttention model.\n",
        "    Args:\n",
        "        d: The embedding dimension.\n",
        "        heads: The number of attention heads.\n",
        "    \"\"\"\n",
        "    def __init__(self, d: int, heads: int=8):\n",
        "        super().__init__()\n",
        "        self.k, self.h = d, heads\n",
        "        \n",
        "        self.Wq = nn.Linear(d, d * heads, bias=False)\n",
        "        self.Wk = nn.Linear(d, d * heads, bias=False)\n",
        "        self.Wv = nn.Linear(d, d * heads, bias=False)\n",
        "        \n",
        "        # This unifies the outputs of the different heads into \n",
        "        # a single k-dimensional vector\n",
        "        self.unifyheads = nn.Linear(heads * d, d)\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: The input embedding of shape [b, l, d].\n",
        "            \n",
        "        Returns:\n",
        "            Self attention tensor of shape [b, l, d].\n",
        "        \"\"\"\n",
        "        b, l, d = x.size()\n",
        "        h = self.h\n",
        "        \n",
        "        # Transform the input embeddings x of shape [b, l, d] to queries, keys, values.\n",
        "        # The output shape is [b, l, d, d*h] which we transform into [b, l, h, d]. Then,\n",
        "        # we fold the heads into the batch dimenstion to arrive at [b*h, l, d]\n",
        "        queries = self.Wq(x).view(b, l, h, d).transpose(1, 2).contiguous().view(b * h, l, d)\n",
        "        keys = self.Wk(x).view(b, l, h, d).transpose(1, 2).contiguous().view(b * h, l, d)\n",
        "        values = self.Wv(x).view(b, l, h, d).transpose(1, 2).contiguous().view(b * h, l, d)\n",
        "        \n",
        "        # Compute the product of queries and keys and scale with sqrt(d).\n",
        "        # The tensor w' has shape (b*h, l, l) containing raw weights.\n",
        "        w_prime = torch.bmm(queries, keys.transpose(1, 2)) / np.sqrt(d)\n",
        "\n",
        "        # Compute w by normalizing w' over the last dimension.\n",
        "        w = F.softmax(w_prime, dim=-1) \n",
        "        \n",
        "        # Apply the self attention to the values.\n",
        "        out = torch.bmm(w, values).view(b, h, l, d)\n",
        "        \n",
        "        # Swap h, l back.\n",
        "        out = out.transpose(1, 2).contiguous().view(b, l, h * d)\n",
        "        \n",
        "        # Unify heads to arrive at shape [b, l, d].\n",
        "        return self.unifyheads(out)\n",
        "  \n",
        "  \n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A Transformer block consisting of self attention and ff-layer.\n",
        "    \n",
        "    Args:\n",
        "        d (int): The embedding dimension.\n",
        "        heads (int): The number of attention heads.\n",
        "        n_mlp (int): The number of mlp 'blocks'.\n",
        "    \"\"\"\n",
        "    def __init__(self, d: int, heads: int=8, n_mlp: int=4):\n",
        "        super().__init__()\n",
        "        \n",
        "        # The self attention layer.\n",
        "        self.attention = SelfAttention(d, heads=heads)\n",
        "        \n",
        "        # The two layer norms.\n",
        "        self.norm1 = nn.LayerNorm(d)\n",
        "        self.norm2 = nn.LayerNorm(d)\n",
        "        \n",
        "        # The feed-forward layer.\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d, n_mlp*d),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_mlp*d, d)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: The input embedding of shape [b, l, d].\n",
        "            \n",
        "        Returns:\n",
        "            Transformer output tensor of shape [b, l, d].\n",
        "        \"\"\"\n",
        "        x_prime = self.attention(x)\n",
        "        x = self.norm1(x_prime + x)\n",
        "        \n",
        "        x_prime = self.ff(x)\n",
        "        return self.norm2(x_prime + x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqoyEb78gazb"
      },
      "source": [
        "# IMDB Movie Reviews Sentiment Classification\n",
        "We train a Transformer classification model to predict the sentiment (positive, negative) of movie reviews in the [IMDB dataset](https://keras.io/api/datasets/imdb/).\n",
        "\n",
        "We use the [pytorch lightning framework](https://pytorch-lightning.readthedocs.io/en/latest/) to massively reduce the amount of code we need to write."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLcNfrbYfdom"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/leox1v/dl20/b3d5b5556d1b2bd360a4abeef4fd82f056ab0301/imgs/classifier.svg?token=AD5WN2R4NHQQTQFOSQHAVWK7VU3JU\" alt=\"drawing\" width=\"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIB-Q_Tkfdoq"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmYhZoFXfdoq"
      },
      "source": [
        "class IMDBDataModule(pl.LightningDataModule):\n",
        "    \"\"\"\n",
        "    LightningDataModule to load the IMDB movie review sentiment data.\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): The batch size for the train, test, and val \n",
        "                            dataloader.\n",
        "    \"\"\" \n",
        "    \n",
        "    def __init__(self, batch_size: int):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "    def setup(self, num_words: int, max_seq_len: int):\n",
        "        \"\"\"\n",
        "        Initial loading of the dataset and transformation.\n",
        "        \n",
        "        Args:\n",
        "            num_words (int): The vocabulary size. The vocabulary is \n",
        "                sorted by frequency of appearance in the dataset.\n",
        "            max_seq_len (int): The maximum number of tokens per\n",
        "                review.\n",
        "        \"\"\"\n",
        "        (self.x_train, self.y_train), (self.x_test, self.y_test) = imdb.load_data(\n",
        "            num_words=num_words, \n",
        "            maxlen=max_seq_len\n",
        "        )\n",
        "        print(f'# Training Examples: {len(self.y_train)}')\n",
        "        print(f'# Test Examples: {len(self.y_test)}')\n",
        "        \n",
        "        self.word2idx = dict(\n",
        "            **{k: v+3 for k, v in imdb.get_word_index().items()},\n",
        "            **{'<PAD>': 0,\n",
        "               '<START>': 1,\n",
        "               '<UNK>': 2,\n",
        "               '<UNUSED>': 3,\n",
        "              },\n",
        "        )\n",
        "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
        "        \n",
        "        # Pad the inputs and convert to torch Tensors.\n",
        "        self.x_train = pad_sequences(self.x_train, maxlen=max_seq_len, value = 0.0)\n",
        "        self.x_test = pad_sequences(self.x_test, maxlen=max_seq_len, value = 0.0)\n",
        "        \n",
        "    \n",
        "    def example(self):\n",
        "        \"\"\"Returns a random training example.\"\"\"        \n",
        "        idx = np.random.randint(0, len(self.x_train))\n",
        "        x, y = self.x_train[idx], self.y_train[idx]\n",
        "        review = ' '.join(self.idx2word[token_id] for token_id in x if token_id > 1)\n",
        "        sentiment = 'POSITIVE' if y else 'NEGATIVE'\n",
        "        return f'{review}\\nSentiment: {sentiment}'\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        dataset = TensorDataset(torch.LongTensor(self.x_train), \n",
        "                                torch.LongTensor(self.y_train))\n",
        "        return DataLoader(dataset, self.batch_size)\n",
        "                                \n",
        "    def test_dataloader(self):\n",
        "        dataset = TensorDataset(torch.LongTensor(self.x_test), \n",
        "                                torch.LongTensor(self.y_test))\n",
        "        return DataLoader(dataset, self.batch_size)\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        return self.test_dataloader()\n",
        "    \n",
        "imdb_data = IMDBDataModule(128)\n",
        "imdb_data.setup(num_words=30000,\n",
        "                max_seq_len=100)\n",
        "print('\\nExamples:')\n",
        "print('\\n\\n'.join(imdb_data.example() for _ in range(3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sJpufM7QkgD"
      },
      "source": [
        "## Transformer Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNoR_7ykfdos"
      },
      "source": [
        "class TextClassificationTransformer(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Stacked Transformer blocks for sequence classification.\n",
        "    \n",
        "    Args:\n",
        "        d (int): The embedding dimension.\n",
        "        heads (int): The number of attention heads for each transformer block.\n",
        "        depth (int): The number of transformer blocks.\n",
        "        max_seq_len (int): The maximum number of tokens of each sequence.\n",
        "        num_tokens (int): The vocabulary size.\n",
        "        num_classes (int): The number of classification classes.\n",
        "        learning_rate (float): The learning rate for the optimizer.\n",
        "    \"\"\"\n",
        "    def __init__(self, d: int=128, heads: int=8, depth: int=6,\n",
        "                max_seq_len: int=512, num_tokens: int=30000, \n",
        "                num_classes: int=2, learning_rate: float=1e-4):\n",
        "        super().__init__()\n",
        "        # Save arguments in self.hparams.\n",
        "        self.save_hyperparameters()\n",
        "        \n",
        "        self.num_tokens = num_tokens\n",
        "        \n",
        "        # Embeddings for tokens and position.\n",
        "        self.token_emb = nn.Embedding(num_tokens, d)\n",
        "        self.pos_emb = nn.Embedding(max_seq_len, d)\n",
        "        \n",
        "        # The stacked transformer blocks.\n",
        "        self.transformer_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(d=d, heads=heads) for _ in range(depth)]\n",
        "        )\n",
        "        \n",
        "        # Mapping of final output sequence to class logits.\n",
        "        self.classification = nn.Linear(d, num_classes)\n",
        "        \n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.accuracy = pl.metrics.Accuracy()\n",
        "        \n",
        "    def forward(self, x: torch.LongTensor) -> torch.FloatTensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (torch.LongTensor): A tensor of shape (b, l) of long values\n",
        "                representing words in some predetermined vocabulary.\n",
        "        \n",
        "        Returns:\n",
        "            A tensor of shape (b, c) of logits over the classes\n",
        "                (c is the number of classes).\n",
        "        \"\"\"\n",
        "        # 1. Generate token embeddings. Shape: [b, l, d].\n",
        "        # 2. Generate position embeddings. Shape: [b, l, d].\n",
        "        # 3. Generate final embedding by taking the sum of the two embeddings.\n",
        "        #----------------\n",
        "        # TODO\n",
        "        #----------------\n",
        "        \n",
        "        # 4. Feed the embedding into the transformer blocks. Shape: [b, l, d].\n",
        "        # 5. Compute the mean latent vector for each sequence.\n",
        "        #    The mean is applied over dim=1 (time). Shape: [b, d].\n",
        "        # 6. Classify. Shape: [b, num_classes].\n",
        "        #----------------\n",
        "        # TODO\n",
        "        #----------------\n",
        "        return out\n",
        "\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"Specify the optimizer used for training.\"\"\"\n",
        "        return Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"A single training step.\n",
        "        \n",
        "        Args:\n",
        "            batch (List(torch.LongTensor, torch.LongTensor)): The current batch\n",
        "                inputs x and sentiment predictions y from the train dataloader.\n",
        "            batch_idx (int): The index of the current batch.\n",
        "        \n",
        "        Returns:\n",
        "            The loss for the current batch.\n",
        "        \"\"\"\n",
        "        x, y = batch\n",
        "        \n",
        "        # Forward pass.\n",
        "        logits = self(x)\n",
        "        \n",
        "        # Compute the loss with CrossEntropy.\n",
        "        loss = self.criterion(logits, y)\n",
        "        \n",
        "        # Log the metrics.\n",
        "        self.log('loss', loss, on_epoch=True, prog_bar=True)\n",
        "        self.log('acc', self.accuracy(logits, y), on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"A single test step.\n",
        "\n",
        "        Pytorch Lightning automatically disable gradients and puts model in eval\n",
        "        mode.\n",
        "        \n",
        "        Args:\n",
        "            batch (List(torch.LongTensor, torch.LongTensor)): The current batch\n",
        "                inputs x and sentiment predictions y from the *test* dataloader.\n",
        "            batch_idx (int): The index of the current batch.\n",
        "        \n",
        "        Returns:\n",
        "            The loss for the current batch.\n",
        "        \"\"\"\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        \n",
        "        # Log the metrics.\n",
        "        self.log('test_loss', loss, on_epoch=True)\n",
        "        self.log('test_acc', self.accuracy(logits, y), on_epoch=True,\n",
        "                 prog_bar=True)\n",
        "        \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"A single validation step.\n",
        "\n",
        "        Since we don't have a seperate validation set we just use the test\n",
        "        logic.\n",
        "        \n",
        "        Args:\n",
        "            batch (List(torch.LongTensor, torch.LongTensor)): The current batch\n",
        "                inputs x and sentiment predictions y from the *val* dataloader.\n",
        "            batch_idx (int): The index of the current batch.\n",
        "        \n",
        "        Returns:\n",
        "            The loss for the current batch.\n",
        "        \"\"\"\n",
        "        return self.test_step(batch, batch_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PONGz69UQqAj"
      },
      "source": [
        "### Train Transformer Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "7s8VE5_zfdou"
      },
      "source": [
        "NUM_WORDS = 10000\n",
        "MAX_SEQ_LEN = 128\n",
        "EMBEDDING_DIM = 128\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "imdb_data = IMDBDataModule(batch_size=BATCH_SIZE)\n",
        "imdb_data.setup(num_words=NUM_WORDS,\n",
        "                max_seq_len=MAX_SEQ_LEN)\n",
        "\n",
        "model = TextClassificationTransformer(d=EMBEDDING_DIM,\n",
        "                                      max_seq_len=MAX_SEQ_LEN,\n",
        "                                      num_tokens=NUM_WORDS)\n",
        "logger = pl.loggers.TensorBoardLogger('tb_logs', name='transformer')\n",
        "trainer = pl.Trainer(max_epochs=5,\n",
        "                     default_root_dir='ckpts',\n",
        "                     gpus=1,\n",
        "                     logger=logger)\n",
        "trainer.fit(model, imdb_data)\n",
        "_ = trainer.test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK7Zqhdl-lR8"
      },
      "source": [
        "## RNN Classification Model\n",
        "Let's compare our Transformer model against an RNN. We want to build a basic stacked RNN architecture as shown in the image below, where we use the output of the final cell to predict the sentiment label. \n",
        "\n",
        "<img src=\"https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment4.png?raw=true\" alt=\"drawing\" width=\"600\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ8Nw_8Q-map"
      },
      "source": [
        "class TextClassificationRNN(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    RNN for sequence classification.\n",
        "    \n",
        "    Args:\n",
        "        d (int): The embedding dimension.\n",
        "        depth (int): The number of RNN blocks.\n",
        "        max_seq_len (int): The maximum number of tokens of each sequence.\n",
        "        num_tokens (int): The vocabulary size.\n",
        "        num_classes (int): The number of classification classes.\n",
        "        learning_rate (float): The learning rate for the optimizer.\n",
        "    \"\"\"\n",
        "    def __init__(self, d: int=128, depth: int=6,\n",
        "                max_seq_len: int=512, num_tokens: int=30000, \n",
        "                num_classes: int=2, learning_rate: float=1e-4):\n",
        "        super().__init__()\n",
        "        # Save arguments in self.hparams.\n",
        "        self.save_hyperparameters()\n",
        "        \n",
        "        self.num_tokens = num_tokens\n",
        "        \n",
        "        # Embeddings for tokens.\n",
        "        self.token_emb = nn.Embedding(num_tokens, d)\n",
        "        \n",
        "        # The stacked GRU layers.\n",
        "        self.rnn = nn.GRU(input_size=d,\n",
        "                          hidden_size=d,\n",
        "                          num_layers=depth,\n",
        "                          batch_first=True)\n",
        "        \n",
        "        # Mapping of final output sequence to class logits.\n",
        "        self.classification = nn.Linear(d, num_classes)\n",
        "        \n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.accuracy = pl.metrics.Accuracy()\n",
        "        \n",
        "    def forward(self, x: torch.LongTensor) -> torch.FloatTensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (torch.LongTensor): A tensor of shape (b, l) of long values\n",
        "                representing words in some predetermined vocabulary.\n",
        "        \n",
        "        Returns:\n",
        "            A tensor of shape (b, c) of logits over the classes\n",
        "                (c is the number of classes).\n",
        "        \"\"\"\n",
        "        # 1. Generate token embeddings (No position embedding required for RNNs). \n",
        "        #     Shape: [b, l, d].\n",
        "        # 2. Feed the embedding into the GRU. Shape: [b, l, d]. Use the output\n",
        "        #     of the last token as the encoding.\n",
        "        # 3. Classify. Shape: [b, num_classes].\n",
        "        #----------------\n",
        "        # TODO\n",
        "        #----------------\n",
        "        return out\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"Specify the optimizer used for training.\"\"\"\n",
        "        return Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"A single training step.\n",
        "        \n",
        "        Args:\n",
        "            batch (List(torch.LongTensor, torch.LongTensor)): The current batch\n",
        "                inputs x and sentiment predictions y from the train dataloader.\n",
        "            batch_idx (int): The index of the current batch.\n",
        "        \n",
        "        Returns:\n",
        "            The loss for the current batch.\n",
        "        \"\"\"\n",
        "        x, y = batch\n",
        "        \n",
        "        # Forward pass.\n",
        "        logits = self(x)\n",
        "        \n",
        "        # Compute the loss with CrossEntropy.\n",
        "        loss = self.criterion(logits, y)\n",
        "        \n",
        "        # Log the metrics.\n",
        "        self.log('loss', loss, on_epoch=True, prog_bar=True)\n",
        "        self.log('acc', self.accuracy(logits, y), on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"A single test step.\n",
        "\n",
        "        Pytorch Lightning automatically disable gradients and puts model in eval\n",
        "        mode.\n",
        "        \n",
        "        Args:\n",
        "            batch (List(torch.LongTensor, torch.LongTensor)): The current batch\n",
        "                inputs x and sentiment predictions y from the *test* dataloader.\n",
        "            batch_idx (int): The index of the current batch.\n",
        "        \n",
        "        Returns:\n",
        "            The loss for the current batch.\n",
        "        \"\"\"\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        \n",
        "        # Log the metrics.\n",
        "        self.log('test_loss', loss, on_epoch=True)\n",
        "        self.log('test_acc', self.accuracy(logits, y), on_epoch=True,\n",
        "                 prog_bar=True)\n",
        "        \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"A single validation step.\n",
        "\n",
        "        Since we don't have a seperate validation set we just use the test\n",
        "        logic.\n",
        "        \n",
        "        Args:\n",
        "            batch (List(torch.LongTensor, torch.LongTensor)): The current batch\n",
        "                inputs x and sentiment predictions y from the *val* dataloader.\n",
        "            batch_idx (int): The index of the current batch.\n",
        "        \n",
        "        Returns:\n",
        "            The loss for the current batch.\n",
        "        \"\"\"\n",
        "        return self.test_step(batch, batch_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVw6zW4CQ20u"
      },
      "source": [
        "### Train RNN Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrCynVgPCXp-"
      },
      "source": [
        "NUM_WORDS = 10000\n",
        "MAX_SEQ_LEN = 128\n",
        "EMBEDDING_DIM = 512\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "imdb_data = IMDBDataModule(batch_size=BATCH_SIZE)\n",
        "imdb_data.setup(num_words=NUM_WORDS,\n",
        "                max_seq_len=MAX_SEQ_LEN)\n",
        "\n",
        "model = TextClassificationRNN(d=EMBEDDING_DIM,\n",
        "                              max_seq_len=MAX_SEQ_LEN,\n",
        "                              num_tokens=NUM_WORDS)\n",
        "logger = pl.loggers.TensorBoardLogger('tb_logs', name='rnn')\n",
        "trainer = pl.Trainer(max_epochs=5,\n",
        "                     default_root_dir='ckpts',\n",
        "                     gpus=1,\n",
        "                     logger=logger)\n",
        "trainer.fit(model, imdb_data)\n",
        "_ = trainer.test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4udeB5bQ8gZ"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOPZRkX1IKKn"
      },
      "source": [
        "%tensorboard --logdir tb_logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}