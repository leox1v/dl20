{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Transformers.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8ad14ee187154bc7b0a6653da8544630": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_91c212a9a1e64652a589f54144a0127a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0d02adc5f9234e19b32c267113d8a804",
              "IPY_MODEL_4949268dd72c4060994d61a3c0b5a337"
            ]
          }
        },
        "91c212a9a1e64652a589f54144a0127a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "0d02adc5f9234e19b32c267113d8a804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ee6319b29e1d4ad5a0370d2fa1783866",
            "_dom_classes": [],
            "description": "Epoch 1: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 87,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 87,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3731a5df76a540e38dd41ecf95585d9e"
          }
        },
        "4949268dd72c4060994d61a3c0b5a337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9025d408e7bb47e29884d5cd6ca44745",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 87/87 [00:03&lt;00:00, 24.07it/s, loss=0.523, v_num=2, loss_step=0.485, acc_step=0.762, loss_epoch=0.703, acc_epoch=0.573]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5caac190063f40edaa22d784d6410202"
          }
        },
        "ee6319b29e1d4ad5a0370d2fa1783866": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3731a5df76a540e38dd41ecf95585d9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9025d408e7bb47e29884d5cd6ca44745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5caac190063f40edaa22d784d6410202": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ff8565087a4f4470a7e3a18f9820b8d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_621757aed9b042c18cb500669ddfd6f0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cc041e91c0c64b6bbc8192e32eb07ce9",
              "IPY_MODEL_74d8f5fbd25c4ad1a1344982a624506e"
            ]
          }
        },
        "621757aed9b042c18cb500669ddfd6f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "cc041e91c0c64b6bbc8192e32eb07ce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_011ac4db16d541af8e8fa95dfb412bf4",
            "_dom_classes": [],
            "description": "Testing: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_49f86b0195cb48a88ae8d1c932de8689"
          }
        },
        "74d8f5fbd25c4ad1a1344982a624506e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a679e3ef4ec34e7a865ddc5046318687",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 93/93 [00:01&lt;00:00, 90.24it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_13aee032478640ea8df3d5734e538139"
          }
        },
        "011ac4db16d541af8e8fa95dfb412bf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "49f86b0195cb48a88ae8d1c932de8689": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a679e3ef4ec34e7a865ddc5046318687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "13aee032478640ea8df3d5734e538139": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSW89sPwgOoD"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmZ529wEgFWi",
        "outputId": "f762a8a1-2aac-4e76-d8e9-933e5ece336b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pytorch_lightning"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.6/dist-packages (1.0.6)\n",
            "Requirement already satisfied: fsspec>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (0.8.4)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (5.3.1)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.18.5)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (0.18.2)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (4.41.1)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (2.3.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch_lightning) (0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch_lightning) (3.7.4.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (50.3.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.7.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.35.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.17.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.12.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.33.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (2.0.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8joHomDfdoR"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import Adam"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRjPv-JffdoW"
      },
      "source": [
        "This tutorial is based on/ copied from http://peterbloem.nl/blog/transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9J7UILOfdoW"
      },
      "source": [
        "# Self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7E4nRIVfdoX"
      },
      "source": [
        "Self-attention maps a sequence of vectors $x_1, ..., x_t$ to an output sequence of vectors $y_1, ..., y_t$ by taking weighted averages of the input:\n",
        "\n",
        "$$y_i = \\sum_j w_{ij}x_j$$\n",
        "\n",
        "Here, $w_{ij}$ captures the interaction between inputs $x_i$ and $x_j$. For example, with the softmax over the inner products, i.e.\n",
        "\n",
        "$$w'_{ij} = x_i^\\text{T}x_j$$\n",
        "\n",
        "$$w_{ij} = \\frac{\\exp(w'_{ij})}{\\sum_jw'_{ij}}$$\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/leox1v/dl20/b3d5b5556d1b2bd360a4abeef4fd82f056ab0301/imgs/self-attention.svg?token=AD5WN2SV46NJT37O73GVHZS7VU2YK\" width=\"600\" valign=\"center\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2szYUZCfdoY",
        "outputId": "c53bb25d-b07d-4007-e09f-ef78855e582f"
      },
      "source": [
        "# Our input x is a sequence of t vectors of dimension k. \n",
        "# Also, we want to process it in a batch of size b later on.\n",
        "# So our dimension is [b, t, k].\n",
        "\n",
        "# Let's start by using a random tensor for x.\n",
        "b, t, l = 8, 4, 10\n",
        "x = torch.rand(size=(b, t, l))\n",
        "print(f'x: {x.shape}')\n",
        "\n",
        "# To compute w', we use the batch matrix multiplication bmm.\n",
        "# This results in dimension [b, t, t].\n",
        "w_prime = torch.bmm(x, x.transpose(1, 2))\n",
        "\n",
        "# By applying the softmax over the last dimension of w_prime, we obtain w.\n",
        "w = F.softmax(w_prime, dim=-1)\n",
        "print(f'w: {w.shape}')\n",
        "\n",
        "# Now to obtain the sequence y (of dimension [b, t, k]), we take the weighted (by w) average of X.\n",
        "y = torch.bmm(w, x)\n",
        "print(f'y: {y.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: torch.Size([8, 4, 10])\n",
            "w: torch.Size([8, 4, 4])\n",
            "y: torch.Size([8, 4, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn6LZkPhfdob"
      },
      "source": [
        "## Query, Key, Value\n",
        "In this basic form of self-attention a single vector $x_i$ is used for three different tasks:\n",
        "1. Used in the weights for its own output $y_i$. -> **query**\n",
        "2. Used in the weights for the j-th output $y_j$. -> **key**\n",
        "3. Used as part of the weighted sum.  -> **value**\n",
        "\n",
        "To disentangle this 3 different 'roles' of $x_i$, we introduce a (learnable) linear transformation for each. In particular, we need 3 $k \\times k$ weight matrices $W_q, W_k, W_v$:\n",
        "\n",
        "$$q_i = W_qx_i \\qquad \\text(Query)$$\n",
        "\n",
        "$$k_i = W_kx_i \\qquad \\text(Key)$$\n",
        "\n",
        "$$v_i = W_vx_i \\qquad \\text(Value)$$\n",
        "\n",
        "This gives the self-attention layer some controllable parameters, and allows it to modify the incoming vectors to suit the three roles they must play.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/leox1v/dl20/b3d5b5556d1b2bd360a4abeef4fd82f056ab0301/imgs/key-query-value.svg?token=AD5WN2VWUZ4MAZY642K5OGK7VU3GQ\" alt=\"drawing\" width=\"500\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_l07F3zfdoc"
      },
      "source": [
        "## Scaling the dot product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4D-174Zfdoc"
      },
      "source": [
        "The softmax function can be sensitive to very large input values. These kill the gradient, and slow down learning. The average value of the dot product grows with the embedding dimension **k**, therefore, it helps to scale the dot product depending on this value:\n",
        "\n",
        "$$w'_{ij}= \\frac{q_i^\\text{T}k_j}{\\sqrt{k}}$$\n",
        "\n",
        "We use $\\sqrt{k}$ in the denominator because that's the euclidean length of a unit vector in $\\mathbb{R}^k$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guL4qzttfdod"
      },
      "source": [
        "## Multi-head attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM6UPFJ4fdod"
      },
      "source": [
        "We can increase the representational power of the self attention by combining them. Instead of using only a single set of 3 transformation matrices $W_q, W_k, W_v$, we use many of them (indexed with $r$) $W^r_q, W^r_k, W^r_v$. These are called *attention heads*.\n",
        "\n",
        "Using the individual attention heads, we produce multiple output vectors $y^r_i$ for a single input vector $x_i$. We can then concatenate the $y^r_i$ vectors and pass them through another linear transformation to reduce the dimension back to $k$.\n",
        "\n",
        "Note for the implementation:\n",
        "While we think about the attention heads as $h$ separate sets of three matrices (of shape $k\\times k$), we implement it by 'stacking' them such that we have only a single set of three matrices of shape $k\\times h*k$. This way we can compute all the concatenated queries, keys, and values in a single matrix multiplication."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI8VK1S6fdod"
      },
      "source": [
        "## Implementation of a SelfAttention Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KH2A6lTfdoe"
      },
      "source": [
        "# Let's implement a SelfAttention torch module.\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A SelfAttention model.\n",
        "    \n",
        "    Args:\n",
        "        k: The embedding dimension.\n",
        "        heads: The number of attention heads.\n",
        "    \"\"\"\n",
        "    def __init__(self, k: int, heads: int=8):\n",
        "        super().__init__()\n",
        "        self.k, self.h = k, heads\n",
        "        \n",
        "        self.Wq = nn.Linear(k, k * heads, bias=False)\n",
        "        self.Wk = nn.Linear(k, k * heads, bias=False)\n",
        "        self.Wv = nn.Linear(k, k * heads, bias=False)\n",
        "        \n",
        "        # This unifies the outputs of the different heads into \n",
        "        # a single k-dimensional vector.\n",
        "        self.unifyheads = nn.Linear(heads * k, k)\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: The input embedding of shape [b, t, k].\n",
        "            \n",
        "        Returns:\n",
        "            Self attention tensor of shape [b, t, k].\n",
        "        \"\"\"\n",
        "        b, t, k = x.size()\n",
        "        h = self.h\n",
        "        \n",
        "        # Transform the input embeddings x of shape [b, t, k] to queries, keys, values.\n",
        "        # The output shape is [b, t, k, k*h] which we transform into [b, t, h, k].\n",
        "        queries = self.Wq(x).view(b, t, h, k)\n",
        "        keys = self.Wk(x).view(b, t, h, k)\n",
        "        values = self.Wv(x).view(b, t, h, k)\n",
        "        \n",
        "        # Fold heads into the batch dimension.\n",
        "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, k)\n",
        "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, k)\n",
        "        values = values.transpose(1, 2).contiguous().view(b * h, t, k)\n",
        "        \n",
        "        # Compute the product of queries and keys and scale with sqrt(k).\n",
        "        # The tensor w' has shape (b*h, t, t) containing raw weights.\n",
        "        w_prime = torch.bmm(queries, keys.transpose(1, 2)) / np.sqrt(k)\n",
        "\n",
        "        # Compute w by normalizing w' over the last dimension.\n",
        "        w = F.softmax(w_prime, dim=-1) \n",
        "        \n",
        "        # Apply the self attention to the values.\n",
        "        out = torch.bmm(w, values).view(b, h, t, k)\n",
        "        \n",
        "        # Swap h, t back.\n",
        "        out = out.transpose(1, 2).contiguous().view(b, t, h * k)\n",
        "        \n",
        "        # Unify heads to arrive at shape [b, t, k].\n",
        "        return self.unifyheads(out)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JaF6C3Dfdog",
        "outputId": "105f9c58-5b8e-43a5-9d3a-25513de56877"
      },
      "source": [
        "# Test it out.\n",
        "b, t, k, h = 2, 4, 6, 8\n",
        "sa = SelfAttention(k=k, heads=h)\n",
        "x = torch.rand(size=(b, t, k))\n",
        "sa(x).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVZJp45ufdoi"
      },
      "source": [
        "# Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFpfKjU4fdoi"
      },
      "source": [
        "The transformer architecture consists of multiple transformer blocks that typically look like this: \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/leox1v/dl20/b3d5b5556d1b2bd360a4abeef4fd82f056ab0301/imgs/transformer-block.svg?token=AD5WN2SZYWM6XGH5SXMZM7S7VU3H4\" alt=\"drawing\" width=\"500\"/>\n",
        "\n",
        "\n",
        "It combines a self attention layer, layer normalization, a feed forward layer and another layer normalization. Additionally, it uses residual connections around the self attention and feed forward layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSQlVDEHfdoj"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A Transformer block consisting of self attention and ff-layer.\n",
        "    \n",
        "    Args:\n",
        "        k (int): The embedding dimension.\n",
        "        heads (int): The number of attention heads.\n",
        "    \"\"\"\n",
        "    def __init__(self, k: int, heads: int=8, n_mlp: int=4):\n",
        "        super().__init__()\n",
        "        \n",
        "        # The self attention layer.\n",
        "        self.attention = SelfAttention(k, heads=heads)\n",
        "        \n",
        "        # The two layer norms.\n",
        "        self.norm1 = nn.LayerNorm(k)\n",
        "        self.norm2 = nn.LayerNorm(k)\n",
        "        \n",
        "        # The feed-forward layer.\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(k, n_mlp*k),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_mlp*k, k)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: The input embedding of shape [b, t, k].\n",
        "            \n",
        "        Returns:\n",
        "            Transformer output tensor of shape [b, t, k].\n",
        "        \"\"\"\n",
        "        x_prime = self.attention(x)\n",
        "        x = self.norm1(x_prime + x)\n",
        "        \n",
        "        x_prime = self.ff(x)\n",
        "        return self.norm2(x_prime + x)\n",
        "        "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r3R2I04fdol"
      },
      "source": [
        "# Text Classification with Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLcNfrbYfdom"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/leox1v/dl20/b3d5b5556d1b2bd360a4abeef4fd82f056ab0301/imgs/classifier.svg?token=AD5WN2R4NHQQTQFOSQHAVWK7VU3JU\" alt=\"drawing\" width=\"800\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCscjHOtfdom"
      },
      "source": [
        "class TextClassificationTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Stacked Transformer blocks for sequence classification.\n",
        "    \n",
        "    Args:\n",
        "        k (int): The embedding dimension.\n",
        "        heads (int): The number of attention heads for each transformer block.\n",
        "        depth (int): The number of transformer blocks.\n",
        "        max_seq_len (int): The maximum number of tokens of each sequence.\n",
        "        num_classes (int): The number of classification classes.\n",
        "    \"\"\"\n",
        "    def __init__(self, k: int, heads: int=8, depth: int=4,\n",
        "                max_seq_len: int=100, num_tokens: int=50000, \n",
        "                num_classes: int=2):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.num_tokens = num_tokens\n",
        "        \n",
        "        # Embeddings for tokens and position.\n",
        "        self.token_emb = nn.Embedding(num_tokens, k)\n",
        "        self.pos_emb = nn.Embedding(max_seq_len, k)\n",
        "        \n",
        "        # The stacked transformer blocks.\n",
        "        self.transformer_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(k=k, heads=heads) for _ in range(depth)]\n",
        "        )\n",
        "        \n",
        "        # Mapping of final output sequence to class logits.\n",
        "        self.classification = nn.Linear(k, num_classes)\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): A tensor of shape (b, t) of integer values\n",
        "                representing words in some predetermined vocabulary.\n",
        "        \n",
        "        Returns:\n",
        "            A tensor of shape (b, c) of logits over the classes\n",
        "                (c is the number of classes).\n",
        "        \"\"\"\n",
        "        # Generate token embeddings.\n",
        "        # Shape: [b, t, k]\n",
        "        tokens = self.token_emb(x)\n",
        "        b, t, k = tokens.size()\n",
        "        \n",
        "        # Generate position embeddings.\n",
        "        # Shape: [b, t, k]\n",
        "        positions = self.pos_emb(torch.arange(t)).unsqueeze(0).expand(b, t, k)\n",
        "        \n",
        "        # Add the two embeddings.\n",
        "        embedding = tokens + positions\n",
        "        \n",
        "        # Feed the embedding into the transformer blocks.\n",
        "        # Shape: [b, t, k]\n",
        "        x = self.transformer_blocks(embedding)\n",
        "        \n",
        "        # Compute the mean latent vector for each sequence.\n",
        "        # The mean is applied over dim=1 (time).\n",
        "        # Shape: [b, k]\n",
        "        x = x.mean(dim=1)\n",
        "        \n",
        "        # Classify.\n",
        "        # Shape: [b, num_classes]\n",
        "        return self.classification(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqoyEb78gazb"
      },
      "source": [
        "# IMDB Movie Reviews Sentiment Classification\n",
        "We train a Transformer classification model to predict the sentiment (positive, negative) of movie reviews in the [IMDB dataset](https://keras.io/api/datasets/imdb/).\n",
        "\n",
        "We use the [pytorch lightning framework](https://pytorch-lightning.readthedocs.io/en/latest/) to massively reduce the amount of code we need to write."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIB-Q_Tkfdoq"
      },
      "source": [
        "### Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmYhZoFXfdoq",
        "outputId": "fcb32179-e99b-4170-ce8b-bdcc3c52964b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "class IMDBDataModule(LightningDataModule):\n",
        "    \"\"\"\n",
        "    LightningDataModule to load the IMDB movie review sentiment data.\n",
        "    \"\"\" \n",
        "    \n",
        "    def __init__(self, batch_size: int):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "    def setup(self, num_words: int, max_seq_len: int):\n",
        "        \"\"\"\n",
        "        Initial loading of the dataset and transformation.\n",
        "        \n",
        "        Args:\n",
        "            num_words (int): The vocabulary size. The vocabulary is \n",
        "                sorted by frequency of appearance in the dataset.\n",
        "            max_seq_len (int): The maximum number of tokens per\n",
        "                review.\n",
        "        \"\"\"\n",
        "        (self.x_train, self.y_train), (self.x_test, self.y_test) = imdb.load_data(\n",
        "            num_words=num_words, \n",
        "            maxlen=max_seq_len\n",
        "        )\n",
        "        print(f'# Training Examples: {len(self.y_train)}')\n",
        "        print(f'# Test Examples: {len(self.y_test)}')\n",
        "        \n",
        "        self.word2idx = dict(\n",
        "            **{k: v+3 for k, v in imdb.get_word_index().items()},\n",
        "            **{'<PAD>': 0,\n",
        "               '<START>': 1,\n",
        "               '<UNK>': 2,\n",
        "               '<UNUSED>': 3,\n",
        "              },\n",
        "        )\n",
        "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
        "        \n",
        "        # Pad the inputs and convert to torch Tensors.\n",
        "        self.x_train = pad_sequences(self.x_train, maxlen=max_seq_len, value = 0.0)\n",
        "        self.x_test = pad_sequences(self.x_test, maxlen=max_seq_len, value = 0.0)\n",
        "    \n",
        "    def example(self):\n",
        "        \"\"\"Returns a random training example.\"\"\"        \n",
        "        idx = np.random.randint(0, len(self.x_train))\n",
        "        x, y = self.x_train[idx], self.y_train[idx]\n",
        "        review = ' '.join(self.idx2word[token_id] for token_id in x if token_id > 1)\n",
        "        sentiment = 'POSITIVE' if y else 'NEGATIVE'\n",
        "        return f'{review}\\nSentiment: {sentiment}'\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        dataset = TensorDataset(torch.LongTensor(self.x_train), \n",
        "                                torch.LongTensor(self.y_train))\n",
        "        return DataLoader(dataset, self.batch_size)\n",
        "                                \n",
        "    def test_dataloader(self):\n",
        "        dataset = TensorDataset(torch.LongTensor(self.x_test), \n",
        "                                torch.LongTensor(self.y_test))\n",
        "        return DataLoader(dataset, self.batch_size)\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        dataset = TensorDataset(torch.LongTensor(self.x_test), \n",
        "                                torch.LongTensor(self.y_test))\n",
        "        return DataLoader(dataset, self.batch_size)\n",
        "    \n",
        "imdb_data = IMDBDataModule(128)\n",
        "imdb_data.setup(num_words=30000,\n",
        "                max_seq_len=100)\n",
        "    "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "# Training Examples: 2773\n",
            "# Test Examples: 2963\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNoR_7ykfdos"
      },
      "source": [
        "# Copy the nn.Module from above and use it as LightningModule here.\n",
        "\n",
        "class TextClassificationTransformer(LightningModule):\n",
        "    \"\"\"\n",
        "    Stacked Transformer blocks for sequence classification.\n",
        "    \n",
        "    Args:\n",
        "        k (int): The embedding dimension.\n",
        "        heads (int): The number of attention heads for each transformer block.\n",
        "        depth (int): The number of transformer blocks.\n",
        "        max_seq_len (int): The maximum number of tokens of each sequence.\n",
        "        num_classes (int): The number of classification classes.\n",
        "    \"\"\"\n",
        "    def __init__(self, k: int, heads: int=8, depth: int=4,\n",
        "                max_seq_len: int=100, num_tokens: int=50000, \n",
        "                num_classes: int=2):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.num_tokens = num_tokens\n",
        "        \n",
        "        # Embeddings for tokens and position.\n",
        "        self.token_emb = nn.Embedding(num_tokens, k)\n",
        "        self.pos_emb = nn.Embedding(max_seq_len, k)\n",
        "        \n",
        "        # The stacked transformer blocks.\n",
        "        self.transformer_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(k=k, heads=heads) for _ in range(depth)]\n",
        "        )\n",
        "        \n",
        "        # Mapping of final output sequence to class logits.\n",
        "        self.classification = nn.Linear(k, num_classes)\n",
        "        \n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.accuracy = pl.metrics.Accuracy()\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): A tensor of shape (b, t) of integer values\n",
        "                representing words in some predetermined vocabulary.\n",
        "        \n",
        "        Returns:\n",
        "            A tensor of shape (b, c) of logits over the classes\n",
        "                (c is the number of classes).\n",
        "        \"\"\"\n",
        "        # Generate token embeddings.\n",
        "        # Shape: [b, t, k]\n",
        "        tokens = self.token_emb(x)\n",
        "        b, t, k = tokens.size()\n",
        "        \n",
        "        # Generate position embeddings.\n",
        "        # Shape: [b, t, k]\n",
        "        positions = self.pos_emb(torch.arange(t).to(self.device)).unsqueeze(0).expand(b, t, k)\n",
        "        \n",
        "        # Add the two embeddings.\n",
        "        embedding = tokens + positions\n",
        "        \n",
        "        # Feed the embedding into the transformer blocks.\n",
        "        # Shape: [b, t, k]\n",
        "        x = self.transformer_blocks(embedding)\n",
        "        \n",
        "        # Compute the mean latent vector for each sequence.\n",
        "        # The mean is applied over dim=1 (time).\n",
        "        # Shape: [b, k]\n",
        "        x = x.mean(dim=1)\n",
        "        \n",
        "        # Classify.\n",
        "        # Shape: [b, num_classes]\n",
        "        return self.classification(x)\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        return Adam(self.parameters(), lr=1e-3)\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        \n",
        "        # Forward pass.\n",
        "        logits = self(x)\n",
        "        \n",
        "        # Compute the loss with CrossEntropy.\n",
        "        loss = self.criterion(logits, y)\n",
        "        \n",
        "        # Log the metrics.\n",
        "        self.log('loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.log('acc', self.accuracy(logits, y), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        # Lightning automatically disables gradients and puts model in eval mode.\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        \n",
        "        # Log the metrics.\n",
        "        self.log('test_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.log('test_acc', self.accuracy(logits, y), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        \n",
        "    def val_step(self, batch, batch_idx):\n",
        "        return self.test_step(batch, batch_idx)\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "7s8VE5_zfdou",
        "outputId": "b9fd5c49-5a73-4e46-ae84-bd8832f80bd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652,
          "referenced_widgets": [
            "8ad14ee187154bc7b0a6653da8544630",
            "91c212a9a1e64652a589f54144a0127a",
            "0d02adc5f9234e19b32c267113d8a804",
            "4949268dd72c4060994d61a3c0b5a337",
            "ee6319b29e1d4ad5a0370d2fa1783866",
            "3731a5df76a540e38dd41ecf95585d9e",
            "9025d408e7bb47e29884d5cd6ca44745",
            "5caac190063f40edaa22d784d6410202",
            "ff8565087a4f4470a7e3a18f9820b8d2",
            "621757aed9b042c18cb500669ddfd6f0",
            "cc041e91c0c64b6bbc8192e32eb07ce9",
            "74d8f5fbd25c4ad1a1344982a624506e",
            "011ac4db16d541af8e8fa95dfb412bf4",
            "49f86b0195cb48a88ae8d1c932de8689",
            "a679e3ef4ec34e7a865ddc5046318687",
            "13aee032478640ea8df3d5734e538139"
          ]
        }
      },
      "source": [
        "NUM_WORDS = 10000\n",
        "MAX_SEQ_LEN = 100\n",
        "EMBEDDING_DIM = 100\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "imdb_data = IMDBDataModule(batch_size=BATCH_SIZE)\n",
        "imdb_data.setup(num_words=NUM_WORDS,\n",
        "                max_seq_len=MAX_SEQ_LEN)\n",
        "\n",
        "model = TextClassificationTransformer(k=EMBEDDING_DIM,\n",
        "                                      max_seq_len=MAX_SEQ_LEN,\n",
        "                                      num_tokens=NUM_WORDS)\n",
        "trainer = pl.Trainer(max_epochs=2,\n",
        "                     default_root_dir='ckpts',\n",
        "                     gpus=1)\n",
        "trainer.fit(model, imdb_data)\n",
        "_ = trainer.test()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: you passed in a val_dataloader but have no validation_step. Skipping validation loop\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name               | Type             | Params\n",
            "--------------------------------------------------------\n",
            "0 | token_emb          | Embedding        | 1 M   \n",
            "1 | pos_emb            | Embedding        | 10 K  \n",
            "2 | transformer_blocks | Sequential       | 1 M   \n",
            "3 | classification     | Linear           | 202   \n",
            "4 | criterion          | CrossEntropyLoss | 0     \n",
            "5 | accuracy           | Accuracy         | 0     \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "# Training Examples: 2773\n",
            "# Test Examples: 2963\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ad14ee187154bc7b0a6653da8544630",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff8565087a4f4470a7e3a18f9820b8d2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'acc': tensor(0.7619, device='cuda:0'),\n",
            " 'acc_epoch': tensor(0.7164, device='cuda:0'),\n",
            " 'acc_step': tensor(0.7619, device='cuda:0'),\n",
            " 'loss': tensor(0.4852, device='cuda:0'),\n",
            " 'loss_epoch': tensor(0.5605, device='cuda:0'),\n",
            " 'loss_step': tensor(0.4852, device='cuda:0'),\n",
            " 'test_acc': tensor(0.6842, device='cuda:0'),\n",
            " 'test_acc_epoch': tensor(0.7175, device='cuda:0'),\n",
            " 'test_loss': tensor(0.5934, device='cuda:0'),\n",
            " 'test_loss_epoch': tensor(0.5746, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}