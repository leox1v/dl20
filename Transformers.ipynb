{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is based on/ copied from http://peterbloem.nl/blog/transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention maps a sequence of vectors $x_1, ..., x_t$ to an output sequence of vectors $y_1, ..., y_t$ by taking weighted averages of the input:\n",
    "\n",
    "$$y_i = \\sum_j w_{ij}x_j$$\n",
    "\n",
    "Here, $w_{ij}$ captures the interaction between inputs $x_i$ and $x_j$. For example, with the softmax over the inner products, i.e.\n",
    "\n",
    "$$w'_{ij} = x_i^\\text{T}x_j$$\n",
    "\n",
    "$$w_{ij} = \\frac{\\exp(w'_{ij})}{\\sum_jw'_{ij}}$$.\n",
    "\n",
    "<img src=\"imgs/self-attention.svg\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([8, 4, 10])\n",
      "w: torch.Size([8, 4, 4])\n",
      "y: torch.Size([8, 4, 10])\n"
     ]
    }
   ],
   "source": [
    "# Our input x is a sequence of t vectors of dimension k. \n",
    "# Also, we want to process it in a batch of size b later on.\n",
    "# So our dimension is [b, t, k].\n",
    "\n",
    "# Let's start by using a random tensor for x.\n",
    "b, t, l = 8, 4, 10\n",
    "x = torch.rand(size=(b, t, l))\n",
    "print(f'x: {x.shape}')\n",
    "\n",
    "# To compute w', we use the batch matrix multiplication bmm.\n",
    "# This results in dimension [b, t, t].\n",
    "w_prime = torch.bmm(x, x.transpose(1, 2))\n",
    "\n",
    "# By applying the softmax over the last dimension of w_prime, we obtain w.\n",
    "w = F.softmax(w_prime, dim=-1)\n",
    "print(f'w: {w.shape}')\n",
    "\n",
    "# Now to obtain the sequence y (of dimension [b, t, k]), we take the weighted (by w) average of X.\n",
    "y = torch.bmm(w, x)\n",
    "print(f'y: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query, Key, Value\n",
    "In this basic form of self-attention a single vector $x_i$ is used for three different tasks:\n",
    "1. Used in the weights for its own output $y_i$. -> **query**\n",
    "2. Used in the weights for the j-th output $y_j$. -> **key**\n",
    "3. Used as part of the weighted sum.  -> **value**\n",
    "\n",
    "To disentangle this 3 different 'roles' of $x_i$, we introduce a (learnable) linear transformation for each. In particular, we need 3 $k \\times k$ weight matrices $W_q, W_k, W_v$:\n",
    "\n",
    "$$q_i = W_qx_i \\qquad \\text(Query)$$\n",
    "\n",
    "$$k_i = W_kx_i \\qquad \\text(Key)$$\n",
    "\n",
    "$$v_i = W_vx_i \\qquad \\text(Value)$$\n",
    "\n",
    "This gives the self-attention layer some controllable parameters, and allows it to modify the incoming vectors to suit the three roles they must play.\n",
    "\n",
    "<img src=\"imgs/key-query-value.svg\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function can be sensitive to very large input values. These kill the gradient, and slow down learning. The average value of the dot product grows with the embedding dimension **k**, therefore, it helps to scale the dot product depending on this value:\n",
    "\n",
    "$$w'_{ij}= \\frac{q_i^\\text{T}k_j}{\\sqrt{k}}$$\n",
    "\n",
    "We use $\\sqrt{k}$ in the denominator because that's the euclidean length of a unit vector in $\\mathbb{R}^k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can increase the representational power of the self attention by combining them. Instead of using only a single set of 3 transformation matrices $W_q, W_k, W_v$, we use many of them (indexed with $r$) $W^r_q, W^r_k, W^r_v$. These are called *attention heads*.\n",
    "\n",
    "Using the individual attention heads, we produce multiple output vectors $y^r_i$ for a single input vector $x_i$. We can then concatenate the $y^r_i$ vectors and pass them through another linear transformation to reduce the dimension back to $k$.\n",
    "\n",
    "Note for the implementation:\n",
    "While we think about the attention heads as $h$ separate sets of three matrices (of shape $k\\times k$), we implement it by 'stacking' them such that we have only a single set of three matrices of shape $k\\times h*k$. This way we can compute all the concatenated queries, keys, and values in a single matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of a SelfAttention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement a SelfAttention torch module.\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A SelfAttention model.\n",
    "    \n",
    "    Args:\n",
    "        k: The embedding dimension.\n",
    "        heads: The number of attention heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, k: int, heads: int=8):\n",
    "        super().__init__()\n",
    "        self.k, self.h = k, heads\n",
    "        \n",
    "        self.Wq = nn.Linear(k, k * heads, bias=False)\n",
    "        self.Wk = nn.Linear(k, k * heads, bias=False)\n",
    "        self.Wv = nn.Linear(k, k * heads, bias=False)\n",
    "        \n",
    "        # This unifies the outputs of the different heads into \n",
    "        # a single k-dimensional vector.\n",
    "        self.unifyheads = nn.Linear(heads * k, k)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: The input embedding of shape [b, t, k].\n",
    "            \n",
    "        Returns:\n",
    "            Self attention tensor of shape [b, t, k].\n",
    "        \"\"\"\n",
    "        b, t, k = x.size()\n",
    "        h = self.h\n",
    "        \n",
    "        # Transform the input embeddings x of shape [b, t, k] to queries, keys, values.\n",
    "        # The output shape is [b, t, k, k*h] which we transform into [b, t, h, k].\n",
    "        queries = self.Wq(x).view(b, t, h, k)\n",
    "        keys = self.Wk(x).view(b, t, h, k)\n",
    "        values = self.Wv(x).view(b, t, h, k)\n",
    "        \n",
    "        # Fold heads into the batch dimension.\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, k)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, k)\n",
    "        values = values.transpose(1, 2).contiguous().view(b * h, t, k)\n",
    "        \n",
    "        # Compute the product of queries and keys and scale with sqrt(k).\n",
    "        # The tensor w' has shape (b*h, t, t) containing raw weights.\n",
    "        w_prime = torch.bmm(queries, keys.transpose(1, 2)) / np.sqrt(k)\n",
    "\n",
    "        # Compute w by normalizing w' over the last dimension.\n",
    "        w = F.softmax(w_prime, dim=-1) \n",
    "        \n",
    "        # Apply the self attention to the values.\n",
    "        out = torch.bmm(w, values).view(b, h, t, k)\n",
    "        \n",
    "        # Swap h, t back.\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t, h * k)\n",
    "        \n",
    "        # Unify heads to arrive at shape [b, t, k].\n",
    "        return self.unifyheads(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 6])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test it out.\n",
    "b, t, k, h = 2, 4, 6, 8\n",
    "sa = SelfAttention(k=k, heads=h)\n",
    "x = torch.rand(size=(b, t, k))\n",
    "sa(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer architecture consists of multiple transformer blocks that typically look like this: \n",
    "\n",
    "<img src=\"imgs/transformer-block.svg\" alt=\"drawing\" width=\"500\"/>\n",
    "It combines a self attention layer, layer normalization, a feed forward layer and another layer normalization. Additionally, it uses residual connections around the self attention and feed forward layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer block consisting of self attention and ff-layer.\n",
    "    \n",
    "    Args:\n",
    "        k (int): The embedding dimension.\n",
    "        heads (int): The number of attention heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, k: int, heads: int=8, n_mlp: int=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # The self attention layer.\n",
    "        self.attention = SelfAttention(k, heads=heads)\n",
    "        \n",
    "        # The two layer norms.\n",
    "        self.norm1 = nn.LayerNorm(k)\n",
    "        self.norm2 = nn.LayerNorm(k)\n",
    "        \n",
    "        # The feed-forward layer.\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(k, n_mlp*k),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_mlp*k, k)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: The input embedding of shape [b, t, k].\n",
    "            \n",
    "        Returns:\n",
    "            Transformer output tensor of shape [b, t, k].\n",
    "        \"\"\"\n",
    "        x_prime = self.attention(x)\n",
    "        x = self.norm1(x_prime + x)\n",
    "        \n",
    "        x_prime = self.ff(x)\n",
    "        return self.norm2(x_prime + x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification with Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/classifier.svg\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Stacked Transformer blocks for sequence classification.\n",
    "    \n",
    "    Args:\n",
    "        k (int): The embedding dimension.\n",
    "        heads (int): The number of attention heads for each transformer block.\n",
    "        depth (int): The number of transformer blocks.\n",
    "        max_seq_len (int): The maximum number of tokens of each sequence.\n",
    "        num_classes (int): The number of classification classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, k: int, heads: int=8, depth: int=4,\n",
    "                max_seq_len: int=100, num_tokens: int=50000, \n",
    "                num_classes: int=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_tokens = num_tokens\n",
    "        \n",
    "        # Embeddings for tokens and position.\n",
    "        self.token_emb = nn.Embedding(num_tokens, k)\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, k)\n",
    "        \n",
    "        # The stacked transformer blocks.\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(k=k, heads=heads) for _ in range(depth)]\n",
    "        )\n",
    "        \n",
    "        # Mapping of final output sequence to class logits.\n",
    "        self.classification = nn.Linear(k, num_classes)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): A tensor of shape (b, t) of integer values\n",
    "                representing words in some predetermined vocabulary.\n",
    "        \n",
    "        Returns:\n",
    "            A tensor of shape (b, c) of logits over the classes\n",
    "                (c is the number of classes).\n",
    "        \"\"\"\n",
    "        # Generate token embeddings.\n",
    "        # Shape: [b, t, k]\n",
    "        tokens = self.token_emb(x)\n",
    "        b, t, k = tokens.size()\n",
    "        \n",
    "        # Generate position embeddings.\n",
    "        # Shape: [b, t, k]\n",
    "        positions = self.pos_emb(torch.arange(t)).unsqueeze(0).expand(b, t, k)\n",
    "        \n",
    "        # Add the two embeddings.\n",
    "        embedding = tokens + positions\n",
    "        \n",
    "        # Feed the embedding into the transformer blocks.\n",
    "        # Shape: [b, t, k]\n",
    "        x = self.transformer_blocks(embedding)\n",
    "        \n",
    "        # Compute the mean latent vector for each sequence.\n",
    "        # The mean is applied over dim=1 (time).\n",
    "        # Shape: [b, k]\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        # Classify.\n",
    "        # Shape: [b, num_classes]\n",
    "        return self.classification(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, LightningDataModule\n",
    "from keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training Examples: 2773\n",
      "# Test Examples: 2963\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class IMDBDataModule(LightningDataModule):\n",
    "    \"\"\"\n",
    "    LightningDataModule to load the IMDB movie review sentiment data.\n",
    "    \"\"\" \n",
    "    \n",
    "    def __init__(self, batch_size: int):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def setup(self, num_words: int, max_seq_len: int):\n",
    "        \"\"\"\n",
    "        Initial loading of the dataset and transformation.\n",
    "        \n",
    "        Args:\n",
    "            num_words (int): The vocabulary size. The vocabulary is \n",
    "                sorted by frequency of appearance in the dataset.\n",
    "            max_seq_len (int): The maximum number of tokens per\n",
    "                review.\n",
    "        \"\"\"\n",
    "        (self.x_train, self.y_train), (self.x_test, self.y_test) = imdb.load_data(\n",
    "            num_words=num_words, \n",
    "            maxlen=max_seq_len\n",
    "        )\n",
    "        print(f'# Training Examples: {len(self.y_train)}')\n",
    "        print(f'# Test Examples: {len(self.y_test)}')\n",
    "        \n",
    "        self.word2idx = dict(\n",
    "            **{k: v+3 for k, v in imdb.get_word_index().items()},\n",
    "            **{'<PAD>': 0,\n",
    "               '<START>': 1,\n",
    "               '<UNK>': 2,\n",
    "               '<UNUSED>': 3,\n",
    "              },\n",
    "        )\n",
    "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
    "        \n",
    "        # Pad the inputs and convert to torch Tensors.\n",
    "        self.x_train = pad_sequences(self.x_train, maxlen=max_seq_len, value = 0.0)\n",
    "        self.x_test = pad_sequences(self.x_test, maxlen=max_seq_len, value = 0.0)\n",
    "    \n",
    "    def example(self):\n",
    "        \"\"\"Returns a random training example.\"\"\"        \n",
    "        idx = np.random.randint(0, len(self.x_train))\n",
    "        x, y = self.x_train[idx], self.y_train[idx]\n",
    "        review = ' '.join(self.idx2word[token_id] for token_id in x if token_id > 1)\n",
    "        sentiment = 'POSITIVE' if y else 'NEGATIVE'\n",
    "        return f'{review}\\nSentiment: {sentiment}'\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataset = TensorDataset(torch.LongTensor(self.x_train), \n",
    "                                torch.LongTensor(self.y_train))\n",
    "        return DataLoader(dataset, self.batch_size)\n",
    "                                \n",
    "    def test_dataloader(self):\n",
    "        dataset = TensorDataset(torch.LongTensor(self.x_test), \n",
    "                                torch.LongTensor(self.y_test))\n",
    "        return DataLoader(dataset, self.batch_size)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        dataset = TensorDataset(torch.LongTensor(self.x_test), \n",
    "                                torch.LongTensor(self.y_test))\n",
    "        return DataLoader(dataset, self.batch_size)\n",
    "    \n",
    "imdb_data = IMDBDataModule(128)\n",
    "imdb_data.setup(num_words=30000,\n",
    "                max_seq_len=100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the nn.Module from above and use it as LightningModule here.\n",
    "\n",
    "class TextClassificationTransformer(LightningModule):\n",
    "    \"\"\"\n",
    "    Stacked Transformer blocks for sequence classification.\n",
    "    \n",
    "    Args:\n",
    "        k (int): The embedding dimension.\n",
    "        heads (int): The number of attention heads for each transformer block.\n",
    "        depth (int): The number of transformer blocks.\n",
    "        max_seq_len (int): The maximum number of tokens of each sequence.\n",
    "        num_classes (int): The number of classification classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, k: int, heads: int=8, depth: int=4,\n",
    "                max_seq_len: int=100, num_tokens: int=50000, \n",
    "                num_classes: int=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_tokens = num_tokens\n",
    "        \n",
    "        # Embeddings for tokens and position.\n",
    "        self.token_emb = nn.Embedding(num_tokens, k)\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, k)\n",
    "        \n",
    "        # The stacked transformer blocks.\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(k=k, heads=heads) for _ in range(depth)]\n",
    "        )\n",
    "        \n",
    "        # Mapping of final output sequence to class logits.\n",
    "        self.classification = nn.Linear(k, num_classes)\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy = pl.metrics.Accuracy()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): A tensor of shape (b, t) of integer values\n",
    "                representing words in some predetermined vocabulary.\n",
    "        \n",
    "        Returns:\n",
    "            A tensor of shape (b, c) of logits over the classes\n",
    "                (c is the number of classes).\n",
    "        \"\"\"\n",
    "        # Generate token embeddings.\n",
    "        # Shape: [b, t, k]\n",
    "        tokens = self.token_emb(x)\n",
    "        b, t, k = tokens.size()\n",
    "        \n",
    "        # Generate position embeddings.\n",
    "        # Shape: [b, t, k]\n",
    "        positions = self.pos_emb(torch.arange(t)).unsqueeze(0).expand(b, t, k)\n",
    "        \n",
    "        # Add the two embeddings.\n",
    "        embedding = tokens + positions\n",
    "        \n",
    "        # Feed the embedding into the transformer blocks.\n",
    "        # Shape: [b, t, k]\n",
    "        x = self.transformer_blocks(embedding)\n",
    "        \n",
    "        # Compute the mean latent vector for each sequence.\n",
    "        # The mean is applied over dim=1 (time).\n",
    "        # Shape: [b, k]\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        # Classify.\n",
    "        # Shape: [b, num_classes]\n",
    "        return self.classification(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        \n",
    "        # Forward pass.\n",
    "        logits = self(x)\n",
    "        \n",
    "        # Compute the loss with CrossEntropy.\n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "        # Log the metrics.\n",
    "        self.log('loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('acc', self.accuracy(logits, y), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Lightning automatically disables gradients and puts model in eval mode.\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "        # Log the metrics.\n",
    "        self.log('test_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('test_acc', self.accuracy(logits, y), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "    def val_step(self, batch, batch_idx):\n",
    "        return self.test_step(batch, batch_idx)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/Users/leox1v/miniconda/envs/dl/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/Users/leox1v/miniconda/envs/dl/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/leox1v/miniconda/envs/dl/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: you passed in a val_dataloader but have no validation_step. Skipping validation loop\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name               | Type             | Params\n",
      "--------------------------------------------------------\n",
      "0 | token_emb          | Embedding        | 3 M   \n",
      "1 | pos_emb            | Embedding        | 10 K  \n",
      "2 | transformer_blocks | Sequential       | 1 M   \n",
      "3 | classification     | Linear           | 202   \n",
      "4 | criterion          | CrossEntropyLoss | 0     \n",
      "5 | accuracy           | Accuracy         | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training Examples: 2773\n",
      "# Test Examples: 2963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leox1v/miniconda/envs/dl/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5cbc6ad74d548aa8e8ca3e2ced2c595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_WORDS = 30000\n",
    "MAX_SEQ_LEN = 100\n",
    "EMBEDDING_DIM = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "imdb_data = IMDBDataModule(batch_size=BATCH_SIZE)\n",
    "imdb_data.setup(num_words=NUM_WORDS,\n",
    "                max_seq_len=MAX_SEQ_LEN)\n",
    "\n",
    "model = TextClassificationTransformer(k=EMBEDDING_DIM,\n",
    "                                      max_seq_len=MAX_SEQ_LEN,\n",
    "                                      num_tokens=NUM_WORDS)\n",
    "trainer = pl.Trainer(max_epochs=2,\n",
    "                     default_root_dir='ckpts')\n",
    "trainer.fit(model, imdb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leox1v/miniconda/envs/dl/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f6edbe92d64754b4a7c1c5632bb333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'acc': tensor(0.2381),\n",
      " 'acc_epoch': tensor(0.5889),\n",
      " 'acc_step': tensor(0.2381),\n",
      " 'loss': tensor(0.8217),\n",
      " 'loss_epoch': tensor(0.6754),\n",
      " 'loss_step': tensor(0.8217),\n",
      " 'test_acc': tensor(0.5263),\n",
      " 'test_acc_epoch': tensor(0.5413),\n",
      " 'test_loss': tensor(0.6920),\n",
      " 'test_loss_epoch': tensor(0.6883)}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'loss_step': 0.8216840028762817,\n",
       "  'acc_step': 0.2380952388048172,\n",
       "  'loss': 0.8216840028762817,\n",
       "  'acc': 0.2380952388048172,\n",
       "  'loss_epoch': 0.6754302978515625,\n",
       "  'acc_epoch': 0.588943600654602,\n",
       "  'test_loss_epoch': 0.6882695555686951,\n",
       "  'test_acc_epoch': 0.5413432121276855,\n",
       "  'test_loss': 0.692012369632721,\n",
       "  'test_acc': 0.5263158082962036}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
