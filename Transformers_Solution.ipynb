{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Transformers.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRjPv-JffdoW"
      },
      "source": [
        "**This tutorial is based on/ copied from http://peterbloem.nl/blog/transformers.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSW89sPwgOoD"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8joHomDfdoR"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9J7UILOfdoW"
      },
      "source": [
        "# Self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7E4nRIVfdoX"
      },
      "source": [
        "Self-attention maps a sequence of vectors $x_1, ..., x_l$ to an output sequence of vectors $y_1, ..., y_l$ by taking weighted averages of the input:\n",
        "\n",
        "$$y_i = \\sum_j w_{ij}x_j$$\n",
        "\n",
        "Here, $w_{ij}$ captures the interaction between inputs $x_i$ and $x_j$. For example, with the softmax over the inner products, i.e.\n",
        "\n",
        "$$w'_{ij} = x_i^\\text{T}x_j$$\n",
        "\n",
        "$$w_{ij} = \\frac{\\exp(w'_{ij})}{\\sum_j\\exp(w'_{ij})}$$\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/leox1v/dl20/b3d5b5556d1b2bd360a4abeef4fd82f056ab0301/imgs/self-attention.svg?token=AD5WN2SV46NJT37O73GVHZS7VU2YK\" width=\"600\" valign=\"center\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2szYUZCfdoY",
        "outputId": "c7e22c2b-0720-48bc-fcee-81b19da47c18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Our input x is a sequence of l vectors of dimension d. \n",
        "# Also, we want to process it in a batch of size b later on.\n",
        "# So our dimension is [b, l, d].\n",
        "\n",
        "# Let's start by using a random tensor for x.\n",
        "b, l, d = 8, 4, 10\n",
        "x = torch.rand(size=(b, l, d))\n",
        "print(f'x: {x.shape}')\n",
        "\n",
        "# To compute w', we use the batch matrix multiplication bmm.\n",
        "# This results in dimension [b, l, l].\n",
        "w_prime = torch.bmm(x, x.transpose(1, 2))\n",
        "\n",
        "# By applying the softmax over the last dimension of w_prime, we obtain w.\n",
        "w = F.softmax(w_prime, dim=-1)\n",
        "print(f'w: {w.shape}')\n",
        "\n",
        "# Now to obtain the sequence y (of dimension [b, l, d]), we take the weighted (by w) average of X.\n",
        "y = torch.bmm(w, x)\n",
        "print(f'y: {y.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: torch.Size([8, 4, 10])\n",
            "w: torch.Size([8, 4, 4])\n",
            "y: torch.Size([8, 4, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn6LZkPhfdob"
      },
      "source": [
        "## Query, Key, Value\n",
        "In this basic form of self-attention a single vector $x_i$ is used for three different tasks:\n",
        "1. Used in the weights for its own output $y_i$. -> **query**\n",
        "2. Used in the weights for the j-th output $y_j$. -> **key**\n",
        "3. Used as part of the weighted sum.  -> **value**\n",
        "\n",
        "To disentangle this 3 different 'roles' of $x_i$, we introduce a (learnable) linear transformation for each. In particular, we need 3 $d \\times d$ weight matrices $W_q, W_k, W_v$:\n",
        "\n",
        "$$q_i = W_qx_i \\qquad \\text(Query)$$\n",
        "\n",
        "$$k_i = W_kx_i \\qquad \\text(Key)$$\n",
        "\n",
        "$$v_i = W_vx_i \\qquad \\text(Value)$$\n",
        "\n",
        "This gives the self-attention layer some controllable parameters, and allows it to modify the incoming vectors to suit the three roles they must play.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/leox1v/dl20/b3d5b5556d1b2bd360a4abeef4fd82f056ab0301/imgs/key-query-value.svg?token=AD5WN2VWUZ4MAZY642K5OGK7VU3GQ\" alt=\"drawing\" width=\"500\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_l07F3zfdoc"
      },
      "source": [
        "## Scaling the dot product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4D-174Zfdoc"
      },
      "source": [
        "The softmax function can be sensitive to very large input values. These kill the gradient, and slow down learning. The average value of the dot product grows with the embedding dimension **d**, therefore, it helps to scale the dot product depending on this value:\n",
        "\n",
        "$$w'_{ij}= \\frac{q_i^\\text{T}k_j}{\\sqrt{d}}$$\n",
        "\n",
        "We use $\\sqrt{d}$ in the denominator because that's the euclidean length of a unit vector in $\\mathbb{R}^d$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guL4qzttfdod"
      },
      "source": [
        "## Multi-head attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM6UPFJ4fdod"
      },
      "source": [
        "We can increase the representational power of the self attention by combining them. Instead of using only a single set of 3 transformation matrices $W_q, W_k, W_v$, we use many of them (indexed with $r$) $W^r_q, W^r_k, W^r_v$. These are called *attention heads*.\n",
        "\n",
        "Using the individual attention heads, we produce multiple output vectors $y^r_i$ for a single input vector $x_i$. We can then concatenate the $y^r_i$ vectors and pass them through another linear transformation to reduce the dimension back to $d$.\n",
        "\n",
        "Note for the implementation:\n",
        "While we think about the attention heads as $h$ separate sets of three matrices (of shape $d\\times d$), we implement it by 'stacking' them such that we have only a single set of three matrices of shape $d\\times h*d$. This way we can compute all the concatenated queries, keys, and values in a single matrix multiplication."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI8VK1S6fdod"
      },
      "source": [
        "## Implementation of a SelfAttention Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KH2A6lTfdoe"
      },
      "source": [
        "# Let's implement a SelfAttention torch module.\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A SelfAttention model.\n",
        "    \n",
        "    Args:\n",
        "        d: The embedding dimension.\n",
        "        heads: The number of attention heads.\n",
        "    \"\"\"\n",
        "    def __init__(self, d: int, heads: int=8):\n",
        "        super().__init__()\n",
        "        self.k, self.h = d, heads\n",
        "        \n",
        "        self.Wq = nn.Linear(d, d * heads, bias=False)\n",
        "        self.Wk = nn.Linear(d, d * heads, bias=False)\n",
        "        self.Wv = nn.Linear(d, d * heads, bias=False)\n",
        "        \n",
        "        # This unifies the outputs of the different heads into \n",
        "        # a single k-dimensional vector.\n",
        "        self.unifyheads = nn.Linear(heads * d, d)\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: The input embedding of shape [b, l, d].\n",
        "            \n",
        "        Returns:\n",
        "            Self attention tensor of shape [b, l, d].\n",
        "        \"\"\"\n",
        "        b, l, d = x.size()\n",
        "        h = self.h\n",
        "        \n",
        "        # Transform the input embeddings x of shape [b, l, d] to queries, keys, values.\n",
        "        # The output shape is [b, l, d*h] which we transform into [b, l, h, d]. Then,\n",
        "        # we fold the heads into the batch dimenstion to arrive at [b*h, l, d]\n",
        "        queries = self.Wq(x).view(b, l, h, d).transpose(1, 2).contiguous().view(b*h, l, d)\n",
        "        keys = self.Wk(x).view(b, l, h, d).transpose(1, 2).contiguous().view(b*h, l, d)\n",
        "        values = self.Wv(x).view(b, l, h, d).transpose(1, 2).contiguous().view(b*h, l, d)\n",
        "        \n",
        "        # Compute the product of queries and keys and scale with sqrt(d).\n",
        "        # The tensor w' has shape (b*h, l, l) containing raw weights.\n",
        "        #----------------\n",
        "        # TODO\n",
        "        w_prime = torch.bmm(queries, keys.transpose(1, 2)) / np.sqrt(d)\n",
        "        #----------------\n",
        "\n",
        "        # Compute w by normalizing w' over the last dimension.\n",
        "        # Shape: [b*h, l, l]\n",
        "        #----------------\n",
        "        # TODO\n",
        "        w = F.softmax(w_prime, dim=-1) \n",
        "        #----------------\n",
        "        \n",
        "        \n",
        "        # Apply the self attention to the values.\n",
        "        # Shape: [b*h, l, d]\n",
        "        #----------------\n",
        "        # TODO\n",
        "        out = torch.bmm(w, values).view(b, h, l, d)\n",
        "        #----------------\n",
        "        \n",
        "        \n",
        "        # Swap h, l back.\n",
        "        # Shape: [b, l, h*d]\n",
        "        out = out.transpose(1, 2).contiguous().view(b, l, h * d)\n",
        "        \n",
        "        # Unify heads to arrive at shape [b, l, d].\n",
        "        return self.unifyheads(out)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JaF6C3Dfdog",
        "outputId": "b37a115a-c863-4264-b881-63553b4d0634",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Test it out.\n",
        "b, l, d, h = 2, 4, 6, 8\n",
        "sa = SelfAttention(d=d, heads=h)\n",
        "x = torch.rand(size=(b, l, d))\n",
        "sa(x).shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVZJp45ufdoi"
      },
      "source": [
        "# Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFpfKjU4fdoi"
      },
      "source": [
        "The transformer architecture consists of multiple transformer blocks that typically look like this: \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/leox1v/dl20/b3d5b5556d1b2bd360a4abeef4fd82f056ab0301/imgs/transformer-block.svg?token=AD5WN2SZYWM6XGH5SXMZM7S7VU3H4\" alt=\"drawing\" width=\"500\"/>\n",
        "\n",
        "\n",
        "It combines a self attention layer, [layer normalization](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html), a feed forward layer and another layer normalization. Additionally, it uses residual connections around the self attention and feed forward layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSQlVDEHfdoj"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A Transformer block consisting of self attention and ff-layer.\n",
        "    \n",
        "    Args:\n",
        "        d (int): The embedding dimension.\n",
        "        heads (int): The number of attention heads.\n",
        "    \"\"\"\n",
        "    def __init__(self, d: int, heads: int=8, n_mlp: int=4):\n",
        "        super().__init__()\n",
        "        \n",
        "        # The self attention layer.\n",
        "        #----------------\n",
        "        # TODO\n",
        "        self.attention = SelfAttention(d, heads=heads)\n",
        "        #----------------\n",
        "        \n",
        "        # The two layer norms.\n",
        "        #----------------\n",
        "        # TODO\n",
        "        self.norm1 = nn.LayerNorm(d)\n",
        "        self.norm2 = nn.LayerNorm(d)\n",
        "        #----------------\n",
        "        \n",
        "        # The feed-forward layer.\n",
        "        #----------------\n",
        "        # TODO\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d, n_mlp*d),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_mlp*d, d)\n",
        "        )\n",
        "        #----------------\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: The input embedding of shape [b, l, d].\n",
        "            \n",
        "        Returns:\n",
        "            Transformer output tensor of shape [b, l, d].\n",
        "        \"\"\"\n",
        "        #----------------\n",
        "        # TODO\n",
        "        x_prime = self.attention(x)\n",
        "        x = self.norm1(x_prime + x)\n",
        "        \n",
        "        x_prime = self.ff(x)\n",
        "        return self.norm2(x_prime + x)\n",
        "        #----------------\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}